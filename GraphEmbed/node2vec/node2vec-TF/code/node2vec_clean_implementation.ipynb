{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import numpy as np\n",
    "import unicodecsv as csv\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os,sys\n",
    "import random\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ref http://stackoverflow.com/questions/8955448/save-load-scipy-sparse-csr-matrix-in-portable-data-format\n",
    "def save_sparse_csr(filename,array):\n",
    "    np.savez(filename,data = array.data ,indices=array.indices,\n",
    "             indptr =array.indptr, shape=array.shape )\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return csr_matrix((  loader['data'], loader['indices'], loader['indptr']),shape = loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "co_author_matrix=load_sparse_csr(\"../data/co-author-matrix.npz\")\n",
    "with open('../data/co-author-index.json', 'r') as f:\n",
    "    aid2aname=json.load(f)\n",
    "aid2aname=dict((int(k), v) for k, v in aid2aname.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74530/74530 [11:21<00:00, 109.31it/s]\n"
     ]
    }
   ],
   "source": [
    "adj_mat_csr_sparse=co_author_matrix\n",
    "\n",
    "def alpha(p,q,t,x):\n",
    "    if t==x:\n",
    "        return 1.0/p\n",
    "    elif adj_mat_csr_sparse[t,x]>0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 1.0/q\n",
    "    \n",
    "p=1.0\n",
    "q=0.5\n",
    "    \n",
    "transition={}\n",
    "\n",
    "num_nodes=adj_mat_csr_sparse.shape[0]\n",
    "indices=adj_mat_csr_sparse.indices\n",
    "indptr=adj_mat_csr_sparse.indptr\n",
    "data=adj_mat_csr_sparse.data\n",
    "\n",
    "#Precompute the transition matrix in advance\n",
    "for t in tqdm(xrange(num_nodes)):#t is row index\n",
    "    for v in indices[indptr[t]:indptr[t+1]]:#i.e  possible next ndoes from t\n",
    "        pi_vx_indices=indices[indptr[v]:indptr[v+1]]#i.e  possible next ndoes from v\n",
    "        pi_vx_values = np.array([alpha(p,q,t,x) for x in pi_vx_indices])\n",
    "        pi_vx_values=pi_vx_values*data[indptr[v]:indptr[v+1]]\n",
    "        #This is eqilvalent to the following\n",
    "#         pi_vx_values=[]\n",
    "#         for x in pi_vx_indices:\n",
    "#             pi_vx=alpha(p,q,t,x)*adj_mat_csr_sparse[v,x]\n",
    "#             pi_vx_values.append(pi_vx)\n",
    "        pi_vx_values=pi_vx_values/np.sum(pi_vx_values)\n",
    "        #now, we have normalzied transion probabilities for v traversed from t\n",
    "        #the probabilities are stored as a sparse vector. \n",
    "        transition[t,v]=(pi_vx_indices,pi_vx_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adj_mat_csr_sparse=co_author_matrix\n",
    "indices=adj_mat_csr_sparse.indices\n",
    "indptr=adj_mat_csr_sparse.indptr\n",
    "data=adj_mat_csr_sparse.data\n",
    "random_walk_length=100\n",
    "    \n",
    "def get_random_walk(p):\n",
    "    random_walks=[]\n",
    "    #get random walks\n",
    "    for u in tqdm(xrange(num_nodes)):\n",
    "        if len(indices[indptr[u]:indptr[u+1]]) !=0:\n",
    "            #first move is just depends on weight\n",
    "            possible_next_node=indices[indptr[u]:indptr[u+1]]\n",
    "            weight_for_next_move=data[indptr[u]:indptr[u+1]]#i.e  possible next ndoes from u\n",
    "            weight_for_next_move=weight_for_next_move.astype(np.float32)/np.sum(weight_for_next_move)\n",
    "            first_walk=np.random.choice(possible_next_node, 1, p=weight_for_next_move)\n",
    "            random_walk=[u,first_walk[0]]\n",
    "            for i in xrange(random_walk_length-2):\n",
    "                cur_node = random_walk[-1]\n",
    "                precious_node=random_walk[-2]\n",
    "                (pi_vx_indices,pi_vx_values)=transition[precious_node,cur_node]\n",
    "                next_node=np.random.choice(pi_vx_indices, 1, p=pi_vx_values)\n",
    "                random_walk.append(next_node[0])\n",
    "            random_walks.append(random_walk)\n",
    "    \n",
    "    return random_walks\n",
    "\n",
    "# random_walks=[]\n",
    "# adj_mat_csr_sparse=co_author_matrix\n",
    "# indices=adj_mat_csr_sparse.indices\n",
    "# indptr=adj_mat_csr_sparse.indptr\n",
    "# data=adj_mat_csr_sparse.data\n",
    "# random_walk_length=100\n",
    "\n",
    "# #get random walks\n",
    "# for u in tqdm(xrange(num_nodes)):\n",
    "#     if len(indices[indptr[u]:indptr[u+1]]) !=0:\n",
    "#         #first move is just depends on weight\n",
    "#         possible_next_node=indices[indptr[u]:indptr[u+1]]\n",
    "#         weight_for_next_move=data[indptr[u]:indptr[u+1]]#i.e  possible next ndoes from u\n",
    "#         weight_for_next_move=weight_for_next_move.astype(np.float32)/np.sum(weight_for_next_move)\n",
    "#         first_walk=np.random.choice(possible_next_node, 1, p=weight_for_next_move)\n",
    "#         random_walk=[u,first_walk[0]]\n",
    "#         for i in xrange(random_walk_length-2):\n",
    "#             cur_node = random_walk[-1]\n",
    "#             precious_node=random_walk[-2]\n",
    "#             (pi_vx_indices,pi_vx_values)=transition[precious_node,cur_node]\n",
    "#             next_node=np.random.choice(pi_vx_indices, 1, p=pi_vx_values)\n",
    "#             random_walk.append(next_node[0])\n",
    "#         random_walks.append(random_walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74530/74530 [07:58<00:00, 155.87it/s]\n",
      "100%|██████████| 74530/74530 [07:58<00:00, 155.71it/s]\n",
      "100%|██████████| 74530/74530 [08:01<00:00, 154.75it/s]\n",
      " 99%|█████████▉| 74151/74530 [08:03<00:02, 158.70it/s]\n",
      "100%|██████████| 74530/74530 [08:03<00:00, 154.04it/s]\n",
      "100%|██████████| 74530/74530 [08:03<00:00, 154.02it/s]\n",
      "100%|██████████| 74530/74530 [08:05<00:00, 153.47it/s]\n",
      "100%|██████████| 74530/74530 [08:05<00:00, 153.45it/s]\n",
      "100%|██████████| 74530/74530 [08:05<00:00, 153.37it/s]\n",
      "100%|██████████| 74530/74530 [08:06<00:00, 153.23it/s]\n",
      "100%|██████████| 74530/74530 [08:08<00:00, 152.63it/s]\n",
      "100%|██████████| 74530/74530 [08:08<00:00, 152.51it/s]\n",
      "100%|██████████| 74530/74530 [08:08<00:00, 152.42it/s]\n",
      "100%|██████████| 74530/74530 [08:11<00:00, 151.61it/s]\n",
      "100%|██████████| 74530/74530 [08:13<00:00, 150.97it/s]\n",
      "100%|██████████| 74530/74530 [08:14<00:00, 150.81it/s]\n",
      "100%|██████████| 74530/74530 [08:16<00:00, 150.02it/s]\n",
      "100%|██████████| 74530/74530 [08:18<00:00, 149.51it/s]\n",
      "100%|██████████| 74530/74530 [08:24<00:00, 147.80it/s]\n",
      "100%|██████████| 74530/74530 [08:26<00:00, 147.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed_time:1350.17567492[sec]\n",
      "elapsed_time:1369.05380893[sec]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "elapsed_time = time.time() - start\n",
    "\n",
    "import multiprocessing as mp\n",
    "proc = 20  \n",
    "pool = mp.Pool(proc)\n",
    "callback = pool.map(get_random_walk, range(20))\n",
    "pool.close()\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "print (\"elapsed_time:{0}\".format(elapsed_time)) + \"[sec]\"\n",
    "\n",
    "random_walks=[]\n",
    "for temp in callback:\n",
    "    random_walks.extend(temp)\n",
    "del callback\n",
    "np_random_walks=np.array(random_walks,dtype=np.int32)\n",
    "del random_walks\n",
    "np.savez('../work/random_walks.npz',np_random_walks)\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "print (\"elapsed_time:{0}\".format(elapsed_time)) + \"[sec]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Computational Graph Definition\n",
    "tf.reset_default_graph()#remove this if not ipython notebook\n",
    "\n",
    "num_nodes=adj_mat_csr_sparse.shape[0]\n",
    "context_size=16\n",
    "batch_size = None\n",
    "embedding_size = 200 # Dimension of the embedding vector.\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# Parameters to learn\n",
    "node_embeddings = tf.Variable(tf.random_uniform([num_nodes, embedding_size], -1.0, 1.0))\n",
    "\n",
    "#Fixedones\n",
    "biases=tf.zeros([num_nodes])\n",
    "\n",
    "# Input data and re-orgenize size.\n",
    "with tf.name_scope(\"context_node\") as scope:\n",
    "    #context nodes to each input node in the batch (e.g [[1,2],[4,6],[5,7]] where batch_size = 3,context_size=3)\n",
    "    train_context_node= tf.placeholder(tf.int32, shape=[batch_size,context_size],name=\"context_node\")\n",
    "    #orgenize prediction labels (skip-gram model predicts context nodes (i.e labels) given a input node)\n",
    "    #i.e make [[1,2,4,6,5,7]] given context above. The redundant dimention is just for restriction on tensorflow API.\n",
    "    train_context_node_flat=tf.reshape(train_context_node,[-1,1])\n",
    "with tf.name_scope(\"input_node\") as scope:\n",
    "    #batch input node to the network(e.g [2,1,3] where batch_size = 3)\n",
    "    train_input_node= tf.placeholder(tf.int32, shape=[batch_size],name=\"input_node\")\n",
    "    #orgenize input as flat. i.e we want to make [2,2,2,1,1,1,3,3,3] given the  input nodes above\n",
    "    input_ones=tf.ones_like(train_context_node)\n",
    "    train_input_node_flat=tf.reshape(tf.mul(input_ones,tf.reshape(train_input_node,[-1,1])),[-1])\n",
    "\n",
    "# Model.\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    # Look up embeddings for words.\n",
    "    node_embed = tf.nn.embedding_lookup(node_embeddings, train_input_node_flat)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss_node2vec = tf.reduce_mean(tf.nn.sampled_softmax_loss(node_embeddings,biases,node_embed,train_context_node_flat, num_sampled, num_nodes))\n",
    "    loss_node2vec_summary = tf.scalar_summary(\"loss_node2vec\", loss_node2vec)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver(max_to_keep=20)\n",
    "\n",
    "# Optimizer.\n",
    "update_loss = tf.train.AdamOptimizer().minimize(loss_node2vec,global_step=global_step)\n",
    "\n",
    "merged = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mcode\u001b[0m/  \u001b[01;34mdata\u001b[0m/  \u001b[01;34mlog_node2vec1\u001b[0m/  \u001b[01;34mresults\u001b[0m/  \u001b[01;34mwork\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: ../log0/model.ckpt-1\n"
     ]
    }
   ],
   "source": [
    "# hyper parameters\n",
    "num_random_walks=np_random_walks.shape[0]\n",
    "\n",
    "# Launch the graph\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    log_dir=\"../log0/\"\n",
    "    writer = tf.train.SummaryWriter(log_dir, sess.graph)\n",
    "    sess.run(init)\n",
    "    for i in xrange(0,num_random_walks):\n",
    "        a_random_walk=np_random_walks[i]\n",
    "        train_input_batch = np.array([a_random_walk[j] for j in xrange(random_walk_length-context_size)])\n",
    "        train_context_batch = np.array([a_random_walk[j+1:j+1+context_size] for j in xrange(random_walk_length-context_size)])\n",
    "        feed_dict={train_input_node:train_input_batch,\n",
    "                   train_context_node:train_context_batch,\n",
    "                  }        \n",
    "        _,loss_value,summary_str=sess.run([update_loss,loss_node2vec,merged], feed_dict)\n",
    "        writer.add_summary(summary_str,i)\n",
    "\n",
    "        with open(log_dir+\"loss_value.txt\", \"a\") as f:\n",
    "            f.write(str(loss_value)+'\\n') \n",
    "                \n",
    "        # Save the variables to disk.\n",
    "        if i%10000==0:\n",
    "            model_path=log_dir+\"model.ckpt\"\n",
    "            save_path = saver.save(sess, model_path,global_step)\n",
    "            print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard --logdir=./log0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
