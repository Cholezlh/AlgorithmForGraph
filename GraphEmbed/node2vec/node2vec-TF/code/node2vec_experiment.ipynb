{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import numpy as np\n",
    "import unicodecsv as csv\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os,sys\n",
    "import random\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of papers:88675\n",
      "The number of authors:102587\n"
     ]
    }
   ],
   "source": [
    "tsv = csv.reader(file('../data/MajorPapers.txt'), delimiter = '\\t')\n",
    "dbpid2pid={}\n",
    "#pid2title={}\n",
    "pid=0\n",
    "for row in tsv:\n",
    "    #row[0] pid \n",
    "    #row[1] title\n",
    "    dbpid=row[0]\n",
    "    dbpid2pid[dbpid]=pid\n",
    "    #pid2title[pid]=nltk.word_tokenize(row[1])\n",
    "    pid+=1\n",
    "\n",
    "print \"The number of papers:%d\"%len(dbpid2pid)\n",
    "\n",
    "#authors\n",
    "tsv = csv.reader(file('../data/MajorAuthors.txt'), delimiter = '\\t')\n",
    "dbaid2aid={}\n",
    "aid2aname={}\n",
    "aid=0\n",
    "for row in tsv:\n",
    "    #row[0] dbaid\n",
    "    #row[1] author name\n",
    "    dbaid=row[0]\n",
    "    aname=row[1]\n",
    "    dbaid2aid[dbaid]=aid\n",
    "    aid2aname[aid]=aname\n",
    "    aid+=1\n",
    "\n",
    "#author-paper\n",
    "tsv = csv.reader(file('../data/MajorPaperAuthor.txt'), delimiter = '\\t')\n",
    "aid2pids={}\n",
    "#iitialize aid2pids\n",
    "for aid in aid2aname:\n",
    "    aid2pids[aid]=[]\n",
    "#collect aids\n",
    "for row in tsv:\n",
    "    #row[0] dbpid \n",
    "    #row[1] dbaid\n",
    "    dbpid=row[0]\n",
    "    pid=dbpid2pid[dbpid]\n",
    "    dbaid=row[1]\n",
    "    aid=dbaid2aid[dbaid]\n",
    "    aid2pids[aid].append(pid)\n",
    "\n",
    "author_paper_indices=[]\n",
    "author_paper_values=[]\n",
    "author_paper_shape=(len(aid2aname), len(dbpid2pid))\n",
    "for aid in aid2pids:\n",
    "    for pid in aid2pids[aid]:\n",
    "        author_paper_indices.append([aid,pid])\n",
    "        author_paper_values.append(1)\n",
    "indeces=np.array(author_paper_indices).T\n",
    "author_paper=csr_matrix((author_paper_values, indeces), shape=author_paper_shape, dtype=np.int32)\n",
    "\n",
    "print \"The number of authors:%d\"%author_paper.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "co_author_matrix=np.dot(author_paper,author_paper.T)\n",
    "for i in xrange(co_author_matrix.shape[0]):\n",
    "    co_author_matrix[i,i]=0\n",
    "co_author_matrix.eliminate_zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ying ding\n",
      "jie tang\n",
      "0\n",
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#print dbaid2aid[\"80CF3CF9\"]Ying\n",
    "print aid2aname[71929]\n",
    "print aid2aname[70282]\n",
    "#print co_author_matrix[71929]\n",
    "print co_author_matrix[70282,70282]\n",
    "print co_author_matrix[71929,70282]\n",
    "print co_author_matrix[70282,71929]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column indices for row i are stored in indices[indptr[i]:indptr[i+1]] and their corresponding values are stored in data[indptr[i]:indptr[i+1]]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102587/102587 [10:51<00:00, 157.42it/s]\n"
     ]
    }
   ],
   "source": [
    "adj_mat_csr_sparse=co_author_matrix\n",
    "\n",
    "def alpha(p,q,t,x):\n",
    "    if t==x:\n",
    "        return 1.0/p\n",
    "    elif adj_mat_csr_sparse[t,x]>0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 1.0/q\n",
    "    \n",
    "p=1.0\n",
    "q=0.5\n",
    "    \n",
    "transition={}\n",
    "\n",
    "num_nodes=adj_mat_csr_sparse.shape[0]\n",
    "indices=adj_mat_csr_sparse.indices\n",
    "indptr=adj_mat_csr_sparse.indptr\n",
    "data=adj_mat_csr_sparse.data\n",
    "\n",
    "#Precompute the transition matrix in advance\n",
    "for t in tqdm(xrange(num_nodes)):#t is row index\n",
    "    for v in indices[indptr[t]:indptr[t+1]]:#i.e  possible next ndoes from t\n",
    "        pi_vx_indices=indices[indptr[v]:indptr[v+1]]#i.e  possible next ndoes from v\n",
    "        pi_vx_values = np.array([alpha(p,q,t,x) for x in pi_vx_indices])\n",
    "        pi_vx_values=pi_vx_values*data[indptr[v]:indptr[v+1]]\n",
    "        #This is eqilvalent to the following\n",
    "#         pi_vx_values=[]\n",
    "#         for x in pi_vx_indices:\n",
    "#             pi_vx=alpha(p,q,t,x)*adj_mat_csr_sparse[v,x]\n",
    "#             pi_vx_values.append(pi_vx)\n",
    "        pi_vx_values=pi_vx_values/np.sum(pi_vx_values)\n",
    "        #now, we have normalzied transion probabilities for v traversed from t\n",
    "        #the probabilities are stored as a sparse vector. \n",
    "        transition[t,v]=(pi_vx_indices,pi_vx_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adj_mat_csr_sparse=co_author_matrix\n",
    "indices=adj_mat_csr_sparse.indices\n",
    "indptr=adj_mat_csr_sparse.indptr\n",
    "data=adj_mat_csr_sparse.data\n",
    "random_walk_length=100\n",
    "    \n",
    "def get_random_walk(p):\n",
    "    random_walks=[]\n",
    "    #get random walks\n",
    "    for u in tqdm(xrange(num_nodes)):\n",
    "        if len(indices[indptr[u]:indptr[u+1]]) !=0:\n",
    "            #first move is just depends on weight\n",
    "            possible_next_node=indices[indptr[u]:indptr[u+1]]\n",
    "            weight_for_next_move=data[indptr[u]:indptr[u+1]]#i.e  possible next ndoes from u\n",
    "            weight_for_next_move=weight_for_next_move.astype(np.float32)/np.sum(weight_for_next_move)\n",
    "            first_walk=np.random.choice(possible_next_node, 1, p=weight_for_next_move)\n",
    "            random_walk=[u,first_walk[0]]\n",
    "            for i in xrange(random_walk_length-2):\n",
    "                cur_node = random_walk[-1]\n",
    "                precious_node=random_walk[-2]\n",
    "                (pi_vx_indices,pi_vx_values)=transition[precious_node,cur_node]\n",
    "                next_node=np.random.choice(pi_vx_indices, 1, p=pi_vx_values)\n",
    "                random_walk.append(next_node[0])\n",
    "            random_walks.append(random_walk)\n",
    "    \n",
    "    return random_walks\n",
    "\n",
    "# random_walks=[]\n",
    "# adj_mat_csr_sparse=co_author_matrix\n",
    "# indices=adj_mat_csr_sparse.indices\n",
    "# indptr=adj_mat_csr_sparse.indptr\n",
    "# data=adj_mat_csr_sparse.data\n",
    "# random_walk_length=100\n",
    "\n",
    "# #get random walks\n",
    "# for u in tqdm(xrange(num_nodes)):\n",
    "#     if len(indices[indptr[u]:indptr[u+1]]) !=0:\n",
    "#         #first move is just depends on weight\n",
    "#         possible_next_node=indices[indptr[u]:indptr[u+1]]\n",
    "#         weight_for_next_move=data[indptr[u]:indptr[u+1]]#i.e  possible next ndoes from u\n",
    "#         weight_for_next_move=weight_for_next_move.astype(np.float32)/np.sum(weight_for_next_move)\n",
    "#         first_walk=np.random.choice(possible_next_node, 1, p=weight_for_next_move)\n",
    "#         random_walk=[u,first_walk[0]]\n",
    "#         for i in xrange(random_walk_length-2):\n",
    "#             cur_node = random_walk[-1]\n",
    "#             precious_node=random_walk[-2]\n",
    "#             (pi_vx_indices,pi_vx_values)=transition[precious_node,cur_node]\n",
    "#             next_node=np.random.choice(pi_vx_indices, 1, p=pi_vx_values)\n",
    "#             random_walk.append(next_node[0])\n",
    "#         random_walks.append(random_walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102587/102587 [09:57<00:00, 171.67it/s]\n",
      "100%|█████████▉| 102211/102587 [10:00<00:02, 182.13it/s]\n",
      "100%|██████████| 102587/102587 [10:01<00:00, 170.51it/s]\n",
      "100%|██████████| 102587/102587 [10:02<00:00, 170.35it/s]\n",
      "100%|██████████| 102587/102587 [10:02<00:00, 170.20it/s]\n",
      "100%|██████████| 102587/102587 [10:04<00:00, 169.81it/s]\n",
      "100%|██████████| 102587/102587 [10:04<00:00, 169.79it/s]\n",
      " 99%|█████████▉| 101988/102587 [10:04<00:03, 195.94it/s]\n",
      "100%|██████████| 102587/102587 [10:05<00:00, 169.37it/s]\n",
      "100%|██████████| 102587/102587 [10:06<00:00, 169.12it/s]\n",
      "100%|██████████| 102587/102587 [10:07<00:00, 188.57it/s]\n",
      "100%|██████████| 102587/102587 [10:08<00:00, 168.63it/s]\n",
      "100%|██████████| 102587/102587 [10:12<00:00, 167.59it/s]\n",
      "100%|██████████| 102587/102587 [10:12<00:00, 167.36it/s]\n",
      "100%|██████████| 102587/102587 [10:16<00:00, 166.35it/s]\n",
      "\n",
      "100%|██████████| 102587/102587 [10:18<00:00, 165.79it/s]\n",
      "100%|██████████| 102587/102587 [10:19<00:00, 165.55it/s]\n",
      "100%|██████████| 102587/102587 [10:22<00:00, 164.83it/s]\n",
      "100%|██████████| 102587/102587 [10:27<00:00, 163.50it/s]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "proc = 20  \n",
    "pool = mp.Pool(proc)\n",
    "callback = pool.map(get_random_walk, range(20))\n",
    "pool.close()\n",
    "random_walks=[]\n",
    "for temp in callback:\n",
    "    random_walks.extend(temp)\n",
    "del callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_random_walks=np.array(random_walks,dtype=np.int32)\n",
    "del random_walks\n",
    "np.save('../work/random_walks.npz',np_random_walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Computational Graph Definition\n",
    "tf.reset_default_graph()#remove this if not ipython notebook\n",
    "\n",
    "num_nodes=adj_mat_csr_sparse.shape[0]\n",
    "context_size=16\n",
    "batch_size = None\n",
    "embedding_size = 200 # Dimension of the embedding vector.\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# Parameters to learn\n",
    "node_embeddings = tf.Variable(tf.random_uniform([num_nodes, embedding_size], -1.0, 1.0))\n",
    "\n",
    "#Fixedones\n",
    "biases=tf.zeros([num_nodes])\n",
    "\n",
    "# Input data and re-orgenize size.\n",
    "with tf.name_scope(\"context_node\") as scope:\n",
    "    #context nodes to each input node in the batch (e.g [[1,2],[4,6],[5,7]] where batch_size = 3,context_size=3)\n",
    "    train_context_node= tf.placeholder(tf.int32, shape=[batch_size,context_size],name=\"context_node\")\n",
    "    #orgenize prediction labels (skip-gram model predicts context nodes (i.e labels) given a input node)\n",
    "    #i.e make [[1,2,4,6,5,7]] given context above. The redundant dimention is just for restriction on tensorflow API.\n",
    "    train_context_node_flat=tf.reshape(train_context_node,[-1,1])\n",
    "with tf.name_scope(\"input_node\") as scope:\n",
    "    #batch input node to the network(e.g [2,1,3] where batch_size = 3)\n",
    "    train_input_node= tf.placeholder(tf.int32, shape=[batch_size],name=\"input_node\")\n",
    "    #orgenize input as flat. i.e we want to make [2,2,2,1,1,1,3,3,3] given the  input nodes above\n",
    "    input_ones=tf.ones_like(train_context_node)\n",
    "    train_input_node_flat=tf.reshape(tf.mul(input_ones,tf.reshape(train_input_node,[-1,1])),[-1])\n",
    "\n",
    "# Model.\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    # Look up embeddings for words.\n",
    "    node_embed = tf.nn.embedding_lookup(node_embeddings, train_input_node_flat)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss_node2vec = tf.reduce_mean(tf.nn.sampled_softmax_loss(node_embeddings,biases,node_embed,train_context_node_flat, num_sampled, num_nodes))\n",
    "    loss_node2vec_summary = tf.scalar_summary(\"loss_node2vec\", loss_node2vec)\n",
    "\n",
    "#logits_cp=tf.matmul(node_embed,node_embeddings_T)\n",
    "#loss_ww=tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits_cp,train_cp_labels,name='xentropy'))\n",
    "#loss_ww_summary = tf.scalar_summary(\"loss_cp\", loss_cp)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver(max_to_keep=20)\n",
    "\n",
    "# Optimizer.\n",
    "update_loss = tf.train.AdamOptimizer().minimize(loss_node2vec,global_step=global_step)\n",
    "\n",
    "merged = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mcode\u001b[0m/  \u001b[01;34mdata\u001b[0m/  \u001b[01;34mlog_node2vec1\u001b[0m/  \u001b[01;34mresults\u001b[0m/  \u001b[01;34mwork\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls ../\n",
    "%rm -rf ../log_node2vec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: ../log_node2vec1/model.ckpt-1\n"
     ]
    }
   ],
   "source": [
    "# hyper parameters\n",
    "num_random_walks=np_random_walks.shape[0]\n",
    "\n",
    "# Launch the graph\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    log_dir=\"../log_node2vec1/\"\n",
    "    writer = tf.train.SummaryWriter(log_dir, sess.graph)\n",
    "    sess.run(init)\n",
    "    for i in xrange(0,num_random_walks):\n",
    "        a_random_walk=np_random_walks[i]\n",
    "        train_input_batch = np.array([a_random_walk[j] for j in xrange(random_walk_length-context_size)])\n",
    "        train_context_batch = np.array([a_random_walk[j+1:j+1+context_size] for j in xrange(random_walk_length-context_size)])\n",
    "        feed_dict={train_input_node:train_input_batch,\n",
    "                   train_context_node:train_context_batch,\n",
    "                  }        \n",
    "        _,loss_value,summary_str=sess.run([update_loss,loss_node2vec,merged], feed_dict)\n",
    "        writer.add_summary(summary_str,i)\n",
    "\n",
    "        with open(log_dir+\"loss_value.txt\", \"a\") as f:\n",
    "            f.write(str(loss_value)+'\\n') \n",
    "                \n",
    "        # Save the variables to disk.\n",
    "        if i%10000==0:\n",
    "            model_path=log_dir+\"model.ckpt\"\n",
    "            save_path = saver.save(sess, model_path,global_step)\n",
    "            print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard --logdir=./log_node2vec1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
